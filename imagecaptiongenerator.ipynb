{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imagecaptiongenerator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNysye4LZCg14CbCSskTMWc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TejasAdhikari/imagecg/blob/main/imagecaptiongenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ii1BBEZWbnY"
      },
      "outputs": [],
      "source": [
        "#Connecting with google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_root = '/content/drive/My Drive/ImageCaptionGenerator'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tfcv1\n",
        "import keras\n",
        "from keras.backend import set_session \n",
        "import sys, time, os, warnings \n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from collections import Counter \n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print(\"python {}\".format(sys.version))\n",
        "print(\"keras version {}\".format(keras.__version__)); del keras\n",
        "print(\"tensorflow version {}\".format(tf.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#configuring the GPU for training\n",
        "config = tfcv1.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.95 #Using 95% of the available memory of the GPU\n",
        "config.gpu_options.visible_device_list = \"0\"\n",
        "set_session(tfcv1.Session(config=config))\n",
        "\n",
        "def set_seed(sd=144):\n",
        "    from numpy.random import seed\n",
        "    from tensorflow import set_random_seed\n",
        "    import random as rn\n",
        "    ## numpy random seed\n",
        "    seed(sd)\n",
        "    ## core python's random number \n",
        "    rn.seed(sd)\n",
        "    ## tensor flow's random number\n",
        "    set_random_seed(sd)"
      ],
      "metadata": {
        "id": "NWrOyh9bgOwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing image and caption dataset\n",
        "# location of the Flickr8K_ images and caption file\n",
        "dir_Flickr_jpg = data_root + \"/Flickr8k_Datasetsubset\"  #/Flickr8k_Dataset\n",
        "dir_Flickr_text = data_root + \"/Flickr8k.token.txt\"\n",
        "\n",
        "jpgs = os.listdir(dir_Flickr_jpg)\n",
        "print(\"The number of jpg flies in Flicker8k: {}\".format(len(jpgs)))"
      ],
      "metadata": {
        "id": "y6zrpJG6jJoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing the caption dataset\n",
        "#Finding the captions for each image.\n",
        "file = open(dir_Flickr_text,'r', encoding='utf8')\n",
        "text = file.read()\n",
        "file.close()\n",
        "\n",
        "datatxt = []\n",
        "for line in text.split('\\n'):\n",
        "    col = line.split('\\t')\n",
        "    if len(col) == 1:\n",
        "        continue\n",
        "    w = col[0].split(\"#\") # Splitting the caption dataset at the required position\n",
        "    datatxt.append(w + [col[1].lower()])\n",
        "\n",
        "df_txt = pd.DataFrame(datatxt,columns=[\"filename\",\"index\",\"caption\"]) #creating the dataframe\n",
        "\n",
        "uni_filenames = np.unique(df_txt.filename.values)\n",
        "print(\"The number of unique file names : {}\".format(len(uni_filenames)))\n",
        "print(\"The distribution of the number of captions for each image:\")\n",
        "Counter(Counter(df_txt.filename.values).values())\n",
        "print(df_txt[:5])"
      ],
      "metadata": {
        "id": "zc0tIQq4B56v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#just plotting the images and their captions in the dataset\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "\n",
        "npic = 5 # Displaying 5 images from the dataset\n",
        "npix = 224\n",
        "target_size = (npix,npix,3)\n",
        "\n",
        "count = 1\n",
        "fig = plt.figure(figsize=(10,20))\n",
        "for jpgfnm in uni_filenames[-5:]:\n",
        "    filename = dir_Flickr_jpg + '/' + jpgfnm\n",
        "    captions = list(df_txt[\"caption\"].loc[df_txt[\"filename\"]==jpgfnm].values)\n",
        "    image_load = load_img(filename, target_size=target_size)\n",
        "    \n",
        "    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n",
        "    ax.imshow(image_load)\n",
        "    count += 1\n",
        "    \n",
        "    ax = fig.add_subplot(npic,2,count)\n",
        "    plt.axis('off')\n",
        "    ax.plot()\n",
        "    ax.set_xlim(0,1)\n",
        "    ax.set_ylim(0,len(captions))\n",
        "    for i, caption in enumerate(captions):\n",
        "        ax.text(0,i,caption,fontsize=20)\n",
        "    count += 1\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sn4fdHPONWNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleaning the captions / removing redundancies\n",
        "#Remove punctuations\n",
        "import string\n",
        "\n",
        "def remove_punctuation(text_original):\n",
        "    text_no_punctuation = text_original.translate(str.maketrans('','',string.punctuation))\n",
        "    return(text_no_punctuation)\n",
        "\n",
        "#Remove single character words\n",
        "def remove_single_character(text):\n",
        "    text_len_more_than1 = \"\"\n",
        "    for word in text.split():\n",
        "        if len(word) > 1:\n",
        "            text_len_more_than1 += \" \" + word\n",
        "    return(text_len_more_than1)\n",
        "\n",
        "#Remove numeric values\n",
        "def remove_numeric(text,printTF=False):\n",
        "    text_no_numeric = \"\"\n",
        "    for word in text.split():\n",
        "        isalpha = word.isalpha()\n",
        "        if isalpha:\n",
        "            text_no_numeric += \" \" + word\n",
        "    return(text_no_numeric)\n",
        "\n",
        "def text_clean(text_original):\n",
        "    text = remove_punctuation(text_original)\n",
        "    text = remove_single_character(text)\n",
        "    text = remove_numeric(text)\n",
        "    return(text)\n",
        "\n",
        "#replacing original captions with new cleaned ones\n",
        "for i, caption in enumerate(df_txt.caption.values):\n",
        "    newcaption = text_clean(caption)\n",
        "    df_txt[\"caption\"].iloc[i] = newcaption"
      ],
      "metadata": {
        "id": "nURWGVhqmGLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to sort the dataframe according to the frequency of the words in the caption dataset\n",
        "def df_word(df_txt):\n",
        "    vocabulary = []\n",
        "    for txt in df_txt.caption.values:\n",
        "        vocabulary.extend(txt.split())\n",
        "    print('Vocabulary Size: %d' % len(set(vocabulary)))\n",
        "    ct = Counter(vocabulary)\n",
        "    dfword = pd.DataFrame({\"word\":list(ct.keys()),\"count\":list(ct.values())})\n",
        "    dfword = dfword.sort_values(\"count\",ascending=False)\n",
        "    dfword = dfword.reset_index()[[\"word\",\"count\"]]\n",
        "    return(dfword)\n",
        "dfword = df_word(df_txt)\n",
        "dfword.head()"
      ],
      "metadata": {
        "id": "xFuLxkTwTY6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plthist(dfsub, title=\"The top 50 most frequently appearing words\"):\n",
        "    plt.figure(figsize=(20,3))\n",
        "    plt.bar(dfsub.index,dfsub[\"count\"])\n",
        "    plt.yticks(fontsize=20)\n",
        "    plt.xticks(dfsub.index,dfsub[\"word\"],rotation=90,fontsize=20)\n",
        "    plt.title(title,fontsize=20)\n",
        "    plt.show()\n",
        "dfword = df_word(df_txt)\n",
        "plthist(dfword.iloc[:50,:], title=\"50 most frequently appearing words\")\n",
        "plthist(dfword.iloc[-50:,:], title=\"50 least frequently appearing words\")"
      ],
      "metadata": {
        "id": "6kb8oGHOxSJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# addding startseq and endseq token for ease of identification \n",
        "# due to variable length of the captions in the dataset\n",
        "from copy import copy\n",
        "def add_start_end_seq_token(captions):\n",
        "    caps = []\n",
        "    for txt in captions:\n",
        "        txt = 'startseq ' + txt + ' endseq'\n",
        "        caps.append(txt)\n",
        "    return(caps)\n",
        "df_txt0 = copy(df_txt)\n",
        "df_txt0[\"caption\"] = add_start_end_seq_token(df_txt[\"caption\"])\n",
        "df_txt0.head(5)\n",
        "del df_txt\n",
        "df_txt0[:5]"
      ],
      "metadata": {
        "id": "WkxV7JvmF3BW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the vgg16 model and removing the last layer \n",
        "#as we just need to extract the features from the images in the dataset\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras import models\n",
        "from keras.models import Sequential\n",
        "\n",
        "modelvgg = VGG16(include_top=True,weights=None)\n",
        "## load the locally saved weights \n",
        "modelvgg.load_weights(data_root + \"/vgg16_weights_tf_dim_ordering_tf_kernels.h5\")\n",
        "modelvgg.summary()\n",
        "\n",
        "# copying the original model to a new model without the last layer\n",
        "modelvgg2 = Sequential()\n",
        "for layer in modelvgg.layers[:-1]:\n",
        "  modelvgg2.add(layer)\n",
        "\n",
        "modelvgg = modelvgg2\n",
        "modelvgg.summary()"
      ],
      "metadata": {
        "id": "8wliq8_VLqON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the features from the images using the modified vgg16 model\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from collections import OrderedDict\n",
        "\n",
        "images = OrderedDict()\n",
        "npix = 224 #image size is fixed at 224 because VGG16 model has been pre-trained to take that size.\n",
        "target_size = (npix,npix,3)\n",
        "data = np.zeros((len(jpgs),npix,npix,3))\n",
        "for i,name in enumerate(jpgs):\n",
        "  # load an image from file\n",
        "  filename = dir_Flickr_jpg + '/' + name\n",
        "  image = load_img(filename, target_size=target_size)\n",
        "  # convert the image pixels to a numpy array\n",
        "  image = img_to_array(image)\n",
        "  nimage = preprocess_input(image)\n",
        "   \n",
        "  y_pred = modelvgg.predict(nimage.reshape( (1,) + nimage.shape[:3]))\n",
        "  images[name] = y_pred.flatten()\n",
        "#image"
      ],
      "metadata": {
        "id": "IrbMkPlDTmCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First the clusters are plotted and few examples are taken from the bunch for displaying\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "encoder = np.array(list(images.values()))\n",
        "#print(encoder)\n",
        "pca = PCA(n_components=2)\n",
        "#print(pca)\n",
        "y_pca = pca.fit_transform(encoder)"
      ],
      "metadata": {
        "id": "cTy2Iwf6-YXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging the images and the captions for training\n",
        "dimages, keepindex = [],[]\n",
        "# Creating a datframe where only first caption is taken for processing\n",
        "df_txt0 = df_txt0.loc[df_txt0[\"index\"].values == \"0\",: ]\n",
        "for i, fnm in enumerate(df_txt0.filename):\n",
        "    if fnm in images.keys():\n",
        "        dimages.append(images[fnm])\n",
        "        keepindex.append(i)\n",
        "\n",
        "#fnames are the names of the image files        \n",
        "fnames = df_txt0[\"filename\"].iloc[keepindex].values\n",
        "#dcaptions are the captions of the images \n",
        "dcaptions = df_txt0[\"caption\"].iloc[keepindex].values\n",
        "#dimages are the actual features of the images\n",
        "dimages = np.array(dimages)\n",
        "df_txt0.head()"
      ],
      "metadata": {
        "id": "x8BCDU9-fH8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the captions as the model can't take texts as an input\n",
        "# hence converting to vectors\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "## the maximum number of words in dictionary\n",
        "nb_words = 6000\n",
        "tokenizer = Tokenizer(nb_words=nb_words)\n",
        "tokenizer.fit_on_texts(dcaptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"vocabulary size : {}\".format(vocab_size))\n",
        "dtexts = tokenizer.texts_to_sequences(dcaptions)\n",
        "print(dtexts[:5])"
      ],
      "metadata": {
        "id": "WfNqV6TIkNw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing, validation n training data split\n",
        "prop_test, prop_val = 0.2, 0.2 \n",
        "\n",
        "N = len(dtexts)\n",
        "Ntest, Nval = int(N*prop_test), int(N*prop_val)\n",
        "\n",
        "\n",
        "def split_test_val_train(dtexts,Ntest,Nval):\n",
        "    return(dtexts[:Ntest], \n",
        "           dtexts[Ntest:Ntest+Nval],  \n",
        "           dtexts[Ntest+Nval:])\n",
        "\n",
        "dt_test,  dt_val, dt_train   = split_test_val_train(dtexts,Ntest,Nval)\n",
        "di_test,  di_val, di_train   = split_test_val_train(dimages,Ntest,Nval)\n",
        "fnm_test,fnm_val, fnm_train  = split_test_val_train(fnames,Ntest,Nval)"
      ],
      "metadata": {
        "id": "FQW0qHjw_VeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing the captions and images as per the required shape by the model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "maxlen = np.max([len(text) for text in dtexts])#finding max length of captions\n",
        "# print(maxlen)\n",
        "\n",
        "\n",
        "def preprocessing(dtexts,dimages):\n",
        "    N = len(dtexts)\n",
        "    print(\"# captions/images = {}\".format(N))\n",
        "\n",
        "    assert(N==len(dimages)) # using assert to make sure that length of images and captions are always same\n",
        "    Xtext, Ximage, ytext = [],[],[]\n",
        "    for text,image in zip(dtexts,dimages):\n",
        "        # zip() is used to create a tuple of iteratable items\n",
        "        for i in range(1,len(text)):\n",
        "            in_text, out_text = text[:i], text[i]\n",
        "            in_text = pad_sequences([in_text],maxlen=maxlen).flatten()# using pad sequence to make the length of all captions equal\n",
        "            out_text = to_categorical(out_text,num_classes = vocab_size) # using to_categorical to \n",
        "      \n",
        "            Xtext.append(in_text)\n",
        "            Ximage.append(image)\n",
        "            ytext.append(out_text)\n",
        "\n",
        "    Xtext  = np.array(Xtext)\n",
        "    Ximage = np.array(Ximage)\n",
        "    ytext  = np.array(ytext)\n",
        "    print(\" {} {} {}\".format(Xtext.shape,Ximage.shape,ytext.shape))\n",
        "    return(Xtext,Ximage,ytext)\n",
        "\n",
        "\n",
        "Xtext_train, Ximage_train, ytext_train = preprocessing(dt_train,di_train)\n",
        "Xtext_val,   Ximage_val,   ytext_val   = preprocessing(dt_val,di_val)\n",
        "# pre-processing is not necessary for testing data\n",
        "#Xtext_test,  Ximage_test,  ytext_test  = preprocessing(dt_test,di_test)"
      ],
      "metadata": {
        "id": "-qhSkL6EJRkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the LSTM model\n",
        "\n",
        "from keras import layers\n",
        "from keras.layers import Input, Flatten, Dropout, Activation\n",
        "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
        "print(vocab_size)\n",
        "## image feature\n",
        "\n",
        "dim_embedding = 64\n",
        "\n",
        "input_image = layers.Input(shape=(Ximage_train.shape[1],))\n",
        "fimage = layers.Dense(256,activation='relu',name=\"ImageFeature\")(input_image)\n",
        "## sequence model\n",
        "input_txt = layers.Input(shape=(maxlen,))\n",
        "ftxt = layers.Embedding(vocab_size,dim_embedding, mask_zero=True)(input_txt)\n",
        "ftxt = layers.LSTM(256,name=\"CaptionFeature\",return_sequences=True)(ftxt)\n",
        "se2 = Dropout(0.04)(ftxt)\n",
        "ftxt = layers.LSTM(256,name=\"CaptionFeature2\")(se2)\n",
        "## combined model for decoder\n",
        "decoder = layers.add([ftxt,fimage])\n",
        "decoder = layers.Dense(256,activation='relu')(decoder)\n",
        "output = layers.Dense(vocab_size,activation='softmax')(decoder)\n",
        "model = models.Model(inputs=[input_image, input_txt],outputs=output)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "DGZYOqVcWo6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the LSTM model\n",
        "from time import time\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
        "hist = model.fit([Ximage_train, Xtext_train], ytext_train, \n",
        "                  epochs=10, verbose=2, \n",
        "                  batch_size=64,\n",
        "                  validation_data=([Ximage_val, Xtext_val], ytext_val),callbacks=[tensorboard])"
      ],
      "metadata": {
        "id": "yrIIEiVDq_YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for label in [\"loss\",\"val_loss\"]:\n",
        "    plt.plot(hist.history[label],label=label)\n",
        "plt.legend()\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O-3pNuZ2p_pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_word = dict([(index,word) for word, index in tokenizer.word_index.items()])\n",
        "def predict_caption(image):\n",
        "    '''\n",
        "    image.shape = (1,4462)\n",
        "    '''\n",
        "\n",
        "    in_text = 'startseq'\n",
        "\n",
        "    for iword in range(maxlen):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence],maxlen)\n",
        "        yhat = model.predict([image,sequence],verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        newword = index_word[yhat]\n",
        "        in_text += \" \" + newword\n",
        "        if newword == \"endseq\":\n",
        "            break\n",
        "    return(in_text)\n",
        "\n",
        "\n",
        "\n",
        "npic = 5\n",
        "npix = 224\n",
        "target_size = (npix,npix,3)\n",
        "\n",
        "count = 1\n",
        "fig = plt.figure(figsize=(10,20))\n",
        "for jpgfnm, image_feature in zip(fnm_test[8:13],di_test[8:13]):\n",
        "    ## images \n",
        "    filename = dir_Flickr_jpg + '/' + jpgfnm\n",
        "    image_load = load_img(filename, target_size=target_size)\n",
        "    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n",
        "    ax.imshow(image_load)\n",
        "    count += 1\n",
        "\n",
        "    ## captions\n",
        "    caption = predict_caption(image_feature.reshape(1,len(image_feature)))\n",
        "    ax = fig.add_subplot(npic,2,count)\n",
        "    plt.axis('off')\n",
        "    ax.plot()\n",
        "    ax.set_xlim(0,1)\n",
        "    ax.set_ylim(0,1)\n",
        "    ax.text(0,0.5,caption,fontsize=20)\n",
        "    count += 1\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "66zpEjQQwbJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "index_word = dict([(index,word) for word, index in tokenizer.word_index.items()])\n",
        "\n",
        "nkeep = 5\n",
        "pred_good, pred_bad, bleus = [], [], [] \n",
        "count = 0 \n",
        "for jpgfnm, image_feature, tokenized_text in zip(fnm_test,di_test,dt_test):\n",
        "    count += 1\n",
        "    if count % 200 == 0:\n",
        "        print(\"  {:4.2f}% is done..\".format(100*count/float(len(fnm_test))))\n",
        "    \n",
        "    caption_true = [ index_word[i] for i in tokenized_text ]     \n",
        "    caption_true = caption_true[1:-1] ## remove startreg, and endreg\n",
        "    ## captions\n",
        "    caption = predict_caption(image_feature.reshape(1,len(image_feature)))\n",
        "    caption = caption.split()\n",
        "    caption = caption[1:-1]## remove startreg, and endreg\n",
        "    \n",
        "    bleu = sentence_bleu([caption_true],caption)\n",
        "    bleus.append(bleu)\n",
        "    if bleu > 0.7 and len(pred_good) < nkeep:\n",
        "        pred_good.append((bleu,jpgfnm,caption_true,caption))\n",
        "    elif bleu < 0.3 and len(pred_bad) < nkeep:\n",
        "        pred_bad.append((bleu,jpgfnm,caption_true,caption))\n",
        "print(\"Mean BLEU {:4.3f}\".format(np.mean(bleus)))"
      ],
      "metadata": {
        "id": "4iRWzkVTycMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_images(pred_bad):\n",
        "    def create_str(caption_true):\n",
        "        strue = \"\"\n",
        "        for s in caption_true:\n",
        "            strue += \" \" + s\n",
        "        return(strue)\n",
        "    npix = 224\n",
        "    target_size = (npix,npix,3)    \n",
        "    count = 1\n",
        "    fig = plt.figure(figsize=(10,20))\n",
        "    npic = len(pred_bad)\n",
        "    for pb in pred_bad:\n",
        "        bleu,jpgfnm,caption_true,caption = pb\n",
        "        ## images \n",
        "        filename = dir_Flickr_jpg + '/' + jpgfnm\n",
        "        image_load = load_img(filename, target_size=target_size)\n",
        "        ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n",
        "        ax.imshow(image_load)\n",
        "        count += 1\n",
        "\n",
        "        caption_true = create_str(caption_true)\n",
        "        caption = create_str(caption)\n",
        "        \n",
        "        ax = fig.add_subplot(npic,2,count)\n",
        "        plt.axis('off')\n",
        "        ax.plot()\n",
        "        ax.set_xlim(0,1)\n",
        "        ax.set_ylim(0,1)\n",
        "        ax.text(0,0.7,\"true:\" + caption_true,fontsize=20)\n",
        "        ax.text(0,0.4,\"pred:\" + caption,fontsize=20)\n",
        "        ax.text(0,0.1,\"BLEU: {}\".format(bleu),fontsize=20)\n",
        "        count += 1\n",
        "    plt.show()\n",
        "\n",
        "print(\"Bad Caption\")\n",
        "plot_images(pred_bad)\n",
        "print(\"Good Caption\")\n",
        "plot_images(pred_good)"
      ],
      "metadata": {
        "id": "ac3P39PxzrCC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}